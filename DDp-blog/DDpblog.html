<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_y5clrjvefv3r-8>li{counter-increment:lst-ctn-kix_y5clrjvefv3r-8}.lst-kix_9gxrnkoii0zj-1>li:before{content:"" counter(lst-ctn-kix_9gxrnkoii0zj-1,lower-latin) ". "}.lst-kix_9gxrnkoii0zj-3>li:before{content:"" counter(lst-ctn-kix_9gxrnkoii0zj-3,decimal) ". "}.lst-kix_nvn15fqesc3j-7>li:before{content:"\0025cb   "}.lst-kix_9gxrnkoii0zj-6>li{counter-increment:lst-ctn-kix_9gxrnkoii0zj-6}ol.lst-kix_9gxrnkoii0zj-4.start{counter-reset:lst-ctn-kix_9gxrnkoii0zj-4 0}.lst-kix_y5clrjvefv3r-2>li{counter-increment:lst-ctn-kix_y5clrjvefv3r-2}.lst-kix_9gxrnkoii0zj-0>li:before{content:"" counter(lst-ctn-kix_9gxrnkoii0zj-0,decimal) ". "}.lst-kix_9gxrnkoii0zj-4>li:before{content:"" counter(lst-ctn-kix_9gxrnkoii0zj-4,lower-latin) ". "}.lst-kix_nvn15fqesc3j-6>li:before{content:"\0025cf   "}ol.lst-kix_9gxrnkoii0zj-2{list-style-type:none}ol.lst-kix_9gxrnkoii0zj-1{list-style-type:none}ol.lst-kix_9gxrnkoii0zj-4{list-style-type:none}ol.lst-kix_9gxrnkoii0zj-3{list-style-type:none}.lst-kix_nvn15fqesc3j-3>li:before{content:"\0025cf   "}.lst-kix_nvn15fqesc3j-5>li:before{content:"\0025a0   "}ol.lst-kix_9gxrnkoii0zj-6{list-style-type:none}ol.lst-kix_9gxrnkoii0zj-5{list-style-type:none}ol.lst-kix_9gxrnkoii0zj-8{list-style-type:none}.lst-kix_9gxrnkoii0zj-2>li:before{content:"" counter(lst-ctn-kix_9gxrnkoii0zj-2,lower-roman) ". "}ol.lst-kix_9gxrnkoii0zj-7{list-style-type:none}.lst-kix_nvn15fqesc3j-4>li:before{content:"\0025cb   "}ol.lst-kix_9gxrnkoii0zj-8.start{counter-reset:lst-ctn-kix_9gxrnkoii0zj-8 0}.lst-kix_9gxrnkoii0zj-7>li{counter-increment:lst-ctn-kix_9gxrnkoii0zj-7}ol.lst-kix_9gxrnkoii0zj-1.start{counter-reset:lst-ctn-kix_9gxrnkoii0zj-1 0}.lst-kix_y5clrjvefv3r-3>li{counter-increment:lst-ctn-kix_y5clrjvefv3r-3}ol.lst-kix_9gxrnkoii0zj-0{list-style-type:none}.lst-kix_9gxrnkoii0zj-8>li:before{content:"" counter(lst-ctn-kix_9gxrnkoii0zj-8,lower-roman) ". "}ol.lst-kix_y5clrjvefv3r-6.start{counter-reset:lst-ctn-kix_y5clrjvefv3r-6 0}.lst-kix_9gxrnkoii0zj-5>li:before{content:"" counter(lst-ctn-kix_9gxrnkoii0zj-5,lower-roman) ". "}.lst-kix_9gxrnkoii0zj-7>li:before{content:"" counter(lst-ctn-kix_9gxrnkoii0zj-7,lower-latin) ". "}.lst-kix_nvn15fqesc3j-8>li:before{content:"\0025a0   "}.lst-kix_9gxrnkoii0zj-0>li{counter-increment:lst-ctn-kix_9gxrnkoii0zj-0}.lst-kix_9gxrnkoii0zj-6>li:before{content:"" counter(lst-ctn-kix_9gxrnkoii0zj-6,decimal) ". "}.lst-kix_y5clrjvefv3r-5>li:before{content:"" counter(lst-ctn-kix_y5clrjvefv3r-5,lower-roman) ". "}ol.lst-kix_9gxrnkoii0zj-5.start{counter-reset:lst-ctn-kix_9gxrnkoii0zj-5 0}.lst-kix_y5clrjvefv3r-4>li:before{content:"" counter(lst-ctn-kix_y5clrjvefv3r-4,lower-latin) ". "}.lst-kix_y5clrjvefv3r-6>li:before{content:"" counter(lst-ctn-kix_y5clrjvefv3r-6,decimal) ". "}ol.lst-kix_y5clrjvefv3r-5.start{counter-reset:lst-ctn-kix_y5clrjvefv3r-5 0}.lst-kix_y5clrjvefv3r-3>li:before{content:"" counter(lst-ctn-kix_y5clrjvefv3r-3,decimal) ". "}.lst-kix_y5clrjvefv3r-7>li:before{content:"" counter(lst-ctn-kix_y5clrjvefv3r-7,lower-latin) ". "}.lst-kix_y5clrjvefv3r-1>li:before{content:"" counter(lst-ctn-kix_y5clrjvefv3r-1,lower-latin) ". "}.lst-kix_y5clrjvefv3r-8>li:before{content:"" counter(lst-ctn-kix_y5clrjvefv3r-8,lower-roman) ". "}.lst-kix_y5clrjvefv3r-0>li:before{content:"" counter(lst-ctn-kix_y5clrjvefv3r-0,decimal) ". "}.lst-kix_y5clrjvefv3r-2>li:before{content:"" counter(lst-ctn-kix_y5clrjvefv3r-2,lower-roman) ". "}.lst-kix_y5clrjvefv3r-0>li{counter-increment:lst-ctn-kix_y5clrjvefv3r-0}.lst-kix_9gxrnkoii0zj-4>li{counter-increment:lst-ctn-kix_9gxrnkoii0zj-4}ul.lst-kix_nvn15fqesc3j-3{list-style-type:none}ul.lst-kix_nvn15fqesc3j-2{list-style-type:none}ol.lst-kix_y5clrjvefv3r-2.start{counter-reset:lst-ctn-kix_y5clrjvefv3r-2 0}ol.lst-kix_y5clrjvefv3r-8.start{counter-reset:lst-ctn-kix_y5clrjvefv3r-8 0}ul.lst-kix_nvn15fqesc3j-5{list-style-type:none}.lst-kix_9gxrnkoii0zj-1>li{counter-increment:lst-ctn-kix_9gxrnkoii0zj-1}ol.lst-kix_9gxrnkoii0zj-2.start{counter-reset:lst-ctn-kix_9gxrnkoii0zj-2 0}ul.lst-kix_nvn15fqesc3j-4{list-style-type:none}ul.lst-kix_nvn15fqesc3j-7{list-style-type:none}ul.lst-kix_nvn15fqesc3j-6{list-style-type:none}ul.lst-kix_nvn15fqesc3j-8{list-style-type:none}ul.lst-kix_nvn15fqesc3j-1{list-style-type:none}ul.lst-kix_nvn15fqesc3j-0{list-style-type:none}.lst-kix_y5clrjvefv3r-5>li{counter-increment:lst-ctn-kix_y5clrjvefv3r-5}ol.lst-kix_y5clrjvefv3r-4.start{counter-reset:lst-ctn-kix_y5clrjvefv3r-4 0}ol.lst-kix_9gxrnkoii0zj-6.start{counter-reset:lst-ctn-kix_9gxrnkoii0zj-6 0}.lst-kix_y5clrjvefv3r-6>li{counter-increment:lst-ctn-kix_y5clrjvefv3r-6}ol.lst-kix_9gxrnkoii0zj-3.start{counter-reset:lst-ctn-kix_9gxrnkoii0zj-3 0}.lst-kix_9gxrnkoii0zj-3>li{counter-increment:lst-ctn-kix_9gxrnkoii0zj-3}ol.lst-kix_y5clrjvefv3r-1.start{counter-reset:lst-ctn-kix_y5clrjvefv3r-1 0}ol.lst-kix_y5clrjvefv3r-1{list-style-type:none}ol.lst-kix_y5clrjvefv3r-0{list-style-type:none}ol.lst-kix_y5clrjvefv3r-3{list-style-type:none}.lst-kix_9gxrnkoii0zj-2>li{counter-increment:lst-ctn-kix_9gxrnkoii0zj-2}ol.lst-kix_y5clrjvefv3r-0.start{counter-reset:lst-ctn-kix_y5clrjvefv3r-0 0}ol.lst-kix_y5clrjvefv3r-2{list-style-type:none}ol.lst-kix_y5clrjvefv3r-5{list-style-type:none}ol.lst-kix_y5clrjvefv3r-4{list-style-type:none}ol.lst-kix_9gxrnkoii0zj-0.start{counter-reset:lst-ctn-kix_9gxrnkoii0zj-0 0}ol.lst-kix_y5clrjvefv3r-3.start{counter-reset:lst-ctn-kix_y5clrjvefv3r-3 0}ol.lst-kix_y5clrjvefv3r-7{list-style-type:none}ol.lst-kix_y5clrjvefv3r-6{list-style-type:none}ol.lst-kix_y5clrjvefv3r-8{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_nvn15fqesc3j-1>li:before{content:"\0025cb   "}ol.lst-kix_y5clrjvefv3r-7.start{counter-reset:lst-ctn-kix_y5clrjvefv3r-7 0}.lst-kix_y5clrjvefv3r-7>li{counter-increment:lst-ctn-kix_y5clrjvefv3r-7}.lst-kix_nvn15fqesc3j-2>li:before{content:"\0025a0   "}.lst-kix_9gxrnkoii0zj-8>li{counter-increment:lst-ctn-kix_9gxrnkoii0zj-8}.lst-kix_y5clrjvefv3r-1>li{counter-increment:lst-ctn-kix_y5clrjvefv3r-1}.lst-kix_9gxrnkoii0zj-5>li{counter-increment:lst-ctn-kix_9gxrnkoii0zj-5}.lst-kix_y5clrjvefv3r-4>li{counter-increment:lst-ctn-kix_y5clrjvefv3r-4}ol.lst-kix_9gxrnkoii0zj-7.start{counter-reset:lst-ctn-kix_9gxrnkoii0zj-7 0}.lst-kix_nvn15fqesc3j-0>li:before{content:"\0025cf   "}ol{margin:0;padding:0}table td,table th{padding:0}.c11{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;margin-right:-36pt;height:11pt}.c10{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c0{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c3{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c2{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c6{padding-top:0pt;padding-bottom:16pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c12{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;margin-right:-36pt}.c1{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c7{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c9{margin-left:36pt;padding-left:0pt}.c5{font-size:14pt;font-weight:700}.c4{padding:0;margin:0}.c13{height:11pt}.c8{font-size:14pt}.c14{margin-left:36pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c7 doc-content"><p class="c10 title" id="h.fpwrby3g7zk9"><span class="c3">Distributed Data Parallel Training</span></p><p class="c6 subtitle" id="h.a3j7x1sj95lf"><span>Augmented Reading</span><span>&nbsp;</span></p><p class="c1"><span class="c2">Have you ever wondered,</span></p><ol class="c4 lst-kix_9gxrnkoii0zj-0 start" start="1"><li class="c12 c9 li-bullet-0"><span class="c2">How do we fit large trillion-parameter models, such as ChatGPT 5.0, that require more than Terabytes of RAM on modern GPUs or APUs, whose RAM is limited to 100-250GB?</span></li></ol><p class="c11 c14"><span class="c2"></span></p><ol class="c4 lst-kix_9gxrnkoii0zj-0" start="2"><li class="c9 c12 li-bullet-0"><span class="c2">How are we reducing training/inference time for models that used to take several months to train and are too slow at responding, like 4-5 years ago</span></li></ol><p class="c11"><span class="c2"></span></p><p class="c12"><span class="c2">To get answers to the above questions, let&rsquo;s dive into the realm of DISTRIBUTED TRAINING. </span></p><p class="c11"><span class="c2"></span></p><p class="c1"><span class="c2">Distributed training is a method used in machine learning to train large, complex models by splitting the computational workload across multiple processors or machines. This approach is necessary when a model or dataset is too large to be handled by a single computer, and it significantly speeds up the training process.</span></p><p class="c1"><span class="c2">There are two primary strategies for distributed training:</span></p><ul class="c4 lst-kix_nvn15fqesc3j-0 start"><li class="c1 c9 li-bullet-0"><span class="c5">Data Parallelism:</span><span class="c2">&nbsp;This is the most common approach. The training dataset is divided into smaller partitions, and a complete copy of the model is placed on each processor (worker node). Each worker trains the model on its unique subset of data. The results (gradients) from all workers are then combined and averaged to update the main model, and the updated model is sent back to all workers for the next iteration.</span></li><li class="c1 c9 li-bullet-0"><span class="c5">Model Parallelism:</span><span class="c2">&nbsp;This strategy is used when the model itself is too large to fit into the memory of a single machine. The model is split into different parts, and each part is placed on a separate machine. All machines work on the same batch of data, with each machine performing calculations for its specific part of the model before passing the results to the next machine in the sequence.</span></li></ul><p class="c12"><span class="c8">Let&#39;s dive deeper into the most common strategy, </span><span class="c5">Distributed Data Parallelism</span><span class="c2">, and unpack how it works step-by-step.</span></p><p class="c1"><span class="c2">As we touched on, the core idea of data parallelism is to replicate the entire model on multiple machines (often called &quot;workers&quot; or &quot;replicas&quot;) and feed each one a different slice of the data. This allows for massive parallel processing. Think of it like having a team of researchers; instead of one person reading a 1,000-page book, you give 10 researchers 100 pages each. They can all read their section simultaneously, drastically reducing the total time required.</span></p><p class="c1 c13"><span class="c2"></span></p><p class="c1"><span class="c2">Here&rsquo;s a breakdown of a typical training loop using Distributed Data Parallelism:</span></p><ol class="c4 lst-kix_y5clrjvefv3r-0 start" start="1"><li class="c1 c9 li-bullet-0"><span class="c5">Initialization &amp; Replication:</span><span class="c2">&nbsp;The process begins with a complete copy of the neural network model being placed on each worker node (e.g., on each GPU). These models are identical, starting with the same initial weights</span></li><li class="c1 c9 li-bullet-0"><span class="c5">Data Sharding:</span><span class="c2">&nbsp;The entire training dataset is split into unique, smaller chunks or &quot;shards.&quot; Each worker is assigned its own shard of the data for the upcoming training epoch.</span></li><li class="c1 c9 li-bullet-0"><span class="c5">Parallel Forward &amp; Backward Pass:</span><span class="c8">&nbsp;All workers simultaneously begin the training process on their respective data shards. Each worker independently performs a </span><span class="c5">forward pass</span><span class="c8">&nbsp;(making predictions) and a </span><span class="c5">backward pass</span><span class="c2">&nbsp;(calculating the error and computing the gradients). At this point, each worker has calculated a set of gradients based only on the data it has seen.</span></li><li class="c1 c9 li-bullet-0"><span class="c5">Gradient Synchronization (The Magic Step):</span><span class="c8">&nbsp;This is the crucial communication phase. The gradients calculated by each individual worker must be combined to get a comprehensive update that represents the entire dataset. This is typically done using an efficient communication algorithm called </span><span class="c5">All-Reduce</span><span class="c2">. The All-Reduce operation gathers the gradients from all workers, averages them, and then distributes this single, averaged gradient back to all the workers.</span></li><li class="c1 c9 li-bullet-0"><span class="c5">Synchronous Model Update:</span><span class="c2">&nbsp;Now that every worker has the exact same averaged gradient, each one independently updates its local copy of the model&#39;s weights. Because they all started with the same model and all received the same averaged gradient, their models remain perfectly in sync after the update.</span></li><li class="c1 c9 li-bullet-0"><span class="c5">Repeat:</span><span class="c2">&nbsp;The entire process repeats for the next batch of data, continuing for many epochs until the model&#39;s performance converges.</span></li></ol><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 316.50px; height: 453.83px;"><img alt="" src="images/image1.png" style="width: 316.50px; height: 453.83px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c8">The primary advantage of this method is its ability to dramatically shorten training times by processing huge datasets in parallel. However, its main challenge is the </span><span class="c5">communication overhead</span><span class="c2">. The All-Reduce step requires significant bandwidth as gradients are sent across the network, and this can sometimes become a bottleneck if not managed efficiently.</span></p></body></html>